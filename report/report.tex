\documentclass[10pt,a4paper]{article}
\usepackage{amssymb,amsmath}
\usepackage{graphicx,subfigure} 	 
\usepackage{times}
\setlength\parindent{0pt}
\title{UG3 Introduction to Vision and Robotics \\ Vision Assignment}
\author{Clemens Wolff, Toms Bergmanis}
\date{March 7th, 2013}

\begin{document}

\maketitle

\section{Introduction}\label{intro}
The goal of this report is to propose an algorithm for object tracking. 
Object tracking, in general, is a challenging problem. Difï¬culties in tracking objects can arise due to abrupt object motion or changing appearance patterns of both the object and its surroundings and other factors. Tracking is usually performed in the context of higher-level applications that require the location and/or shape of the object in every frame. Typically, assumptions are made to constrain the tracking problem in the context of a particular application\cite{REF}.


\section{Methods}\label{methods}

Several simplifying assumptions were made to constrain the tracking problem.
It was assumed that:
\begin{enumerate}
\item objects to be tracked will be puck-like robots and that they will each appear in different shades of red or blue, or green.
\item camera will be set up in an angle not less than 45\% with respect to the plane it is observing. 
\item on the top of the robot will be a triangle indicating it's direction and that it will be in darker colour than the colour indicating the robot.  
\item background of the image will be in colour other than any of the colours of the robots.
\end{enumerate}



\subsection{Detection of Robots}\label{coloralgo}
\begin{description}
\item[Input] \hfill \\
    $I$, a three channel image of dimensions $m \times n$ in the RGB colorspace.
\item[Output] \hfill \\
    $M$, a $m \times n \times 3$ binary matrix where for each pixel $P{ij}$ of
    $I$, it holds that: \\
    $M(i,j,1) = 1 \leftrightarrow P_{ij}$ belongs to the red robot, \\
    $M(i,j,2) = 1 \leftrightarrow P_{ij}$ belongs to the green robot, \\
    $M(i,j,3) = 1 \leftrightarrow P_{ij}$ belongs to the blue robot.
\end{description}
\textbf{Algorithm}
\begin{enumerate}
    \item
    Apply approximate RGB-normalisation to $I$, giving $I_n$:
    \begin{itemize}
        \item
        For each pixel in $I_n$, calculate the sum $S_{rgb}$ of the red, green, 
        and blue values of that pixel.
        \item
        If $S_{rgb} \ne 0$ (the pixel is not absolute black), set each of the 
        pixel's red, green, and blue values to that value divided by $S_{rgb}$.
    \end{itemize}
    \item
    Calculate $\mu_r, \mu_g, \mu_b$ and $\sigma_r, \sigma_g, \sigma_b$, the 
    means and standard deviations of the values in the three channels of $I_n$.
    \item
    Assign each pixel $P_{ij}$ in $I$ to one of the robots or to the background:
    \begin{itemize}
        \item
        Normalise $P$'s red, green, and blue values, giving $P_n$.
        \item
        Calculate the probabilities $p_r, p_g, p_b$ that $P_n$ was generated 
        by the gaussian distributions $\mathcal{N}_r = (\mu_r, \sigma_r),
        \mathcal{N}_g = (\mu_g, \sigma_g), \mathcal{N}_b = (\mu_b, \sigma_b)$.
        \item
        Calculate $P$'s hue value $h$.
        \item
        If $h$ is whithin a certain range defined as red and $p_r$ is
        sufficiently small, set $M(i,j,1) = 1$ (similarly for ranges defined as
        green/blue and $p_g$/$p_b$. If none of these conditions are met, set
        $M(i,j,1) = M(i,j,2) = M(i,j,3) = 0$.
    \end{itemize}
    \item
    Remove noise from each channel in $M$:
    \begin{itemize}
        \item
        Set pixels to zero if they have fewer neighbours with value one than
        they have adjacent pixels with value zero.
        \item
        Set zero-valued pixels to one if they have two one-valued 
        horizontal or vertical neighbours.
    \end{itemize}
    \item
    Remove components that are distant from the main concentration of mass in
    each channel in $M$:
    \begin{itemize}
        \item
        Compute the center of mass $C$ of the channel.
        \item
        Compute $c_1, c_2, \ldots$, the centers of mass of each connected 
        component in the channel.
        \item
        Compute the mean distance $\delta$ of the $c_k$ to $C$.
        \item
        Set $M(i,j) = 0$ for all the pixels $(i,j)$ in the components $k$ that
        satisfy $c_k > \tau \delta$ for some fixed threshold $\tau$.
    \end{itemize}
    \item
    Exploit the fact that all robots have similar sizes by setting every channel
    in $M$ to all-zeros if the number of pixels set in that channel is smaller
    than the number of pixels set in the most populated channel by some margin.
\end{enumerate} 
\begin{figure}[ht]
    \centering
    \includegraphics[width=40mm]{d1_i5_blob_mask.jpg}
    \caption{Result of Robot detection}
    \label{colorfig}
\end{figure} 
Figure \ref{colorfig} shows a visualisation of the output matrix $M$.

\subsection{Finding Robot Directions}\label{directionalgo}
\begin{description}
\item[Input] \hfill \\
    $I$, a three channel image of dimensions $m \times n$ in the RGB colorspace.
\item[Output] \hfill \\
    $\Lambda = \{(c^m_r, c^t_r), (c^m_g, c^t_g), (c^m_b, c^t_b)\}$, a set where
    $c^m_r$ is the center of mass of the red robot and $c^t_r$ is the point 
    towards which the robot is facing (similarly for the green and blue robots).
\end{description}
\textbf{Algorithm}
\begin{enumerate}
    \item
    Get a matrix of robot masks $M$ using the algorithm in Section 
    \ref{coloralgo}. Let $M_i$ be the i$^{th}$ channel of $M$ i.e. the set of
    points $\{M(a,b,i) | 1 \le a \le m, 1 \le b \le n\}$.\\
    Apply the remainder of the algorithm to each channel $\xi$ in $M$.
    \item
    Calculate the convex hull $H$ of the points in the channel and create the 
    set of pixels of $I$ that are inside $H$: $P = \{p_{ij} | p_{ij} \in \xi 
    \land M(i,j,\xi) = 1\}$
    \item
    Calculate $\mu$, the average rgb-value over $P$. Generate 
    $\Pi = \{p | p \in P \land rgbvalue(p) < \mu\}$, the set of pixels in $P$ 
    that have a below-average rgb value.
    \item
    The black triangles on the robots are the pixels in $\Pi$. Get rid of them
    by setting the relevant indices in $M$ to zero.\\
    Recompute the convex hull of $M$.
    \begin{figure}[ht]
        \begin{center}
            \subfigure[After Removing Triangles]{
                \includegraphics[width=40mm]{d1_i5_demasked_triangles.jpg}}
            \subfigure[Removed Triangles]{
                \includegraphics[width=40mm]{d1_i5_triangles.jpg}}
        \end{center}
        \caption{Triangle Detection via Local Thresholding}
        \label{trianglefig}
    \end{figure} 
    \item
    Repeat the previous step and remember the pixels in $\Pi$. \\
    This reduces noise in $M$ by giving a tighter estimate on the robot's
    pixels when the triangles were under-detected by the algorithm in 
    Section \ref{coloralgo}. \\
    Figure \ref{trianglefig} shows the result of this step - a notable
    improvement in clarity of the triangles compared to Figure \ref{colorfig}.
    \item
    Update $\Lambda$: $c^m_\xi$ is the center of mass of $M_\xi$, $c^t_\xi$ is 
    the center of mass of $\Pi$. \\
    A line from $c^m_\xi$ to $c^t_\xi$ indicates the direction of the robot.
\end{enumerate} 
\begin{figure}[ht]
    \centering
    \includegraphics[width=40mm]{d1_i5_result.jpg}
    \caption{Detected Directions}
    \label{directionfig}
\end{figure} 
Figure \ref{directionfig} shows a visualisation of the output set $\Lambda$.

\subsection{Tracking Robots Over a Sequences of Frames}\label{trackingalgo}
\begin{description}
\item[Input] \hfill \\
    $\Upsilon = \{I_1, I_2, \cdots\}$, a sequence where each of the $I_i$ is a 
    three channel image of dimensions $m \times n$ in the RGB colorspace.
\item[Output] \hfill \\
    $\Omega$, a visualisation of the robot positions over $\Upsilon$.
\end{description}
\textbf{Algorithm}
\begin{enumerate}
    \item
    Use a median-filter to generate a background $\Omega$ from $\Upsilon$. \\
    For each $1 \le i \le m, 1 \le j \le n$:
    \begin{itemize}
        \item
        Create $\omega_{ij} = \{I_k(i,j) | I_k \in \Upsilon\}$, the set of the 
        colors of the pixels at location $(i,j)$ of all the images in 
        $\Upsilon$.
        \item
        Set $\Omega(i,j) = median(\omega_{ij})$.
    \end{itemize}
    \item
    For each $I_i \in \Upsilon$:
    \begin{itemize}
        \item
        Use the algorithm in Section \ref{directionalgo} to get the set 
        $\Lambda$. Let $\lambda = \{c | (c, \_) \in \Lambda\}$.
        \item
        Overlay $\Omega$ with a line from each element in $\lambda_{i-1}$ to 
        the corresponding element in $\lambda_i$, thus linking the centroids 
        from image $I_{i-1}$ to the centroids in image $I_i$.
    \end{itemize}
\end{enumerate} 
Figure \ref{trackingfig} shows a visualisation of the resulting track.
\begin{figure}[ht]
    \begin{center}
    \subfigure[Dataset 1]{
        \includegraphics[height=40mm]{d1_trace.jpg}}
    \subfigure[Dataset 2]{
        \includegraphics[height=40mm]{d2_trace.jpg}}
    \end{center}
    \caption{Output of Tracing Algorithm}
    \label{trackingfig}
\end{figure}


\section{Results}\label{results}
This section evaluates and visualises the performance of the three algorithms 
presented in Sections \ref{coloralgo}, \ref{directionalgo}, and 
\ref{trackingalgo}. Table \ref{datasetpropstable} describes the properties of 
the datasets used for this evaluation.
\begin{table}[ht]
\begin{tabular}{|l || l | l | p{2cm} | p{4cm}|}
\hline
\# & Background & Robot Size & Robot Color & Illumination \\
\hline
1 & uniform, gray & large & saturated, dark & uniform, red hue \\
\hline
2 & noisy, gray & small & faded, blue robot is cyan & 
    histograms are bell-shaped \\
\hline
3 & patterned, brown & large & saturated & daylight only \\
\hline
4 & patterned, brown & large & saturated & daylight and artificial 
    light \\
\hline
\end{tabular}
\caption{Properties of evaluation datasets}
\label{datasetpropstable}
\end{table}

\subsection{Detection of Robots}\label{colorresults}
The algorithm described in Section \ref{coloralgo}, worked perfectly on 
datasets 1 and 2.\\
Evaluation on the third dataset led to the worst performance over all datasets, 
with $\sim$60\% of the occurences of the blue robot being undetected and 
$\sim$40\% of the occurences of the green robot being under-detected (leading 
to bad direction detection).\\
The performance on the fourth dataset was interesting: the red robot was under-
detected in $\sim$45\% of the cases (with the blue and green robots being
found just fine) - while in the other datasets the red robot was usually
detected with the highest confidence.
Over all four datasets, about 10\% of the robot instances were badly detected.

\subsection{Detection of Directions}\label{directionresults}
Performance of detection of the directions was heavily dependent on the 
performance of the detection of the robots. In case of precisely detected robot 
detected direction perfectly matched the actual direction. 
In case of loose detection - detection where some addition non-robot region is 
misleadingly detected as a robot - detected direction perfectly matched the 
actual direction due to algorithms ability to filter noisy detections. 
In case of under-detection - detection where some parts of the image 
representing the robot where omitted - detected direction was skewed on the 
side opposite (from the axis matching the actual robot's direction) to the 
misdirected fragment of the robot. Error was proportional to the error of 
under-detected area of the robot. 
\begin{figure}[ht]
    \begin{center}
        \subfigure[Loose hull detection]{
            \includegraphics[width=40mm]{d2_i32_loose_detection_example.jpg}}
        \subfigure[Hull under detection]{
            \includegraphics[width=40mm]{d4_i66_bad_detection_example.jpg}}
        \subfigure[Good hull detection]{
            \includegraphics[width=40mm]{d1_i100_good_detection_example.jpg}}
    \end{center}
    \caption{Direction detections for convex hulls of different qualities}
\end{figure} 

\subsection{Tracking of the robots}\label{trackingresults}
Section \ref{trackingalgo}'s algorithm to track robots over consecutive frames
is trivial - a mere visualisation of half of the results of the robot-direction-
detection algorithm presented in Section \ref{directionalgo}. The tracking 
algorithm's performance is therefore directly related to the performance of 
the robot-direction-detection algorithm and the same observations as in Section
\ref{directionresults} apply: generally speaking, the algorithm performed 
well.\\
Figure \ref{trackingfig} begets one additional observation related to the
evaluation of the robot-tracking algorithm: both datasets considered in this
report exhibit the property that one of the objects of interest does not move
much for most of the frames. This entails that generating a background from
data employing a simple frame-difference base approach (such as the median-
filter used in Section \ref{trackingalgo}) to perform background subtraction 
is bound to  fail as one of the objects of interest will be considered a part 
of the background due to being mostly stationary. This is unfortunate since
pre-processing the datasets with background subtraction would increase the
accuracy of Section \ref{coloralgo}'s algorithm by reducing noise and increasing
resolution in the image.


\section{Discussion}\label{discussion}
Overall performance of the algorithm can be evaluated as good. It operates 
under little or no simplifying assumptions.
Robot detection could be improved [WRITE PLEASE HOW]
Another way how to improve the detection of the robots could be utilisation of 
second order spatial statistics of the robot images. Using such approach might 
give good results combined with our current approach.
Direction detection could be improved by making it less dependent on detection 
of the robots. It could be achieved by performing local search near the regions 
detected as robots. Such approach would improve accuracy of the directions 
detected, however it would make algorithm much slower. 
Another possible direction of development could be shape analysis - currently 
algorithms utilise only image's colour statistics due to limited information 
about the scale of the images and the possible placements of the camera. 


\section{Code}\label{code}
the new Matlab code that you developed for this assignment. Do not
include code that you downloaded from the course web pages. Any other code
that you downloaded should be recorded in the report, but does not need to
be included in the appendix.

\bibliographystyle{IEEEtran}
\begin{thebibliography}{10}
\bibitem{REF} Yilmaz, A., Javed, O., and Shah, M., 
``Object Tracking: A Survey'', 
ACM Comput. Surv. 38, 4, Article 13, 2006.  
\end{thebibliography}
\end{document}
